{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The ActionFinder \n",
    "## (Ibata, Diakogiannis, Famaey & Monari, 2021)\n",
    "\n",
    "# An unsupervised network to find the canonical transformation\n",
    "# from x,v to J,theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probably don't want to fiddle with these:\n",
    "dtype = torch.float64 # data type to use\n",
    "np_dtype = np.float64 # same for numpy\n",
    "\n",
    "num_workers=1         # most recent version of pytorch complains if we ask for more data workers !!\n",
    "\n",
    "NPhase =  6           # number of phase-space coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The scale factors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_scale =  20.0   # will scale all input distances  by this factor\n",
    "vel_scale = 200.0   # will scale all input velocities by this factor\n",
    "J_scale   = pos_scale*vel_scale\n",
    "T_scale   = vel_scale/pos_scale\n",
    "\n",
    "# loss function inverse scales:\n",
    "scale_Ts      =  10.0 # (rough) between Js and Ts\n",
    "scale_Js_same =   1.0 # to increase importance of identical Js\n",
    "scale_direc   = 0.001 # for the acceleration direction constraint\n",
    "scale_Os_same = 100.0 # to increase importance of identical Omegas\n",
    "scale_dJdt    = 100.0 # for dJdt\n",
    "scale_LinAl   =1000.0 # to increase importance of linear algebra solution for acceleration\n",
    "\n",
    "\n",
    "# The reference isochrone model (these values are a good starting point for the Milky Way):\n",
    "M_iso_ref=1500000.0/(pos_scale*vel_scale**2) # masses in N-body units (i.e. 1.5e6*2.222e5=3.333e11 Msun)\n",
    "b_iso_ref=5.0/pos_scale                      # scale in N-body units (i.e. 5 kpc)\n",
    "\n",
    "\n",
    "torch.pi = torch.acos(torch.zeros(1)).item() * 2\n",
    "torch.J_scale = torch.tensor(J_scale)\n",
    "torch.W_scale=torch.zeros(6)\n",
    "torch.W_scale[:3]=pos_scale\n",
    "torch.W_scale[3:]=vel_scale\n",
    "W_scale=np.zeros(6)\n",
    "W_scale[:3]=pos_scale\n",
    "W_scale[3:]=vel_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the choices for this run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Epochs = 1024*2                  # Number of epochs to run for\n",
    "\n",
    "batch_size=128*4\n",
    "ndata_select=batch_size*2        # training   dataset size\n",
    "ndata_select_test=batch_size*2   # validation dataset size\n",
    "\n",
    "\n",
    "genname_root = 'generate_orbits0'\n",
    "\n",
    "fitsdata_name = '../data/'+genname_root+'.fits'\n",
    "flname_save = r'../saved_models/ActionFinder0.params'\n",
    "\n",
    "\n",
    "# file to make predictions for, and output file with predictions:\n",
    "fitspred_name =     r'../data/generate_orbits0.fits'\n",
    "fitspred_name_out = r'../data/generate_orbits0_withPreds.fits'\n",
    "\n",
    "\n",
    "NStars_per_stream = 8 # NOTE: The fits file has NPhase*NStars_per_stream columns\n",
    "\n",
    "# number of stars that we wish to treat as one stream group (these will be randomly drawn from NStars_per_stream)\n",
    "NStars = 8 # The idea is that NStars << NStars_per_stream, so \n",
    "\n",
    "assert NStars <= NStars_per_stream, \"We cannot have NStars > NStars_per_stream\"\n",
    "\n",
    "\n",
    "# Half-depths of the neural networks:\n",
    "depth_GF_G = 7\n",
    "depth_GF_P = 5\n",
    "depth_A    = 7\n",
    "\n",
    "\n",
    "# Switches:\n",
    "Fit_Acceleration = False             # set to True on second-pass to get the acceleration field.\n",
    "use_point_transformation = True      # do we use the optional point-transformation network?\n",
    "use_validation_set = True            # the network is unsupervised, so validation is not critical if we don't want to propagate solutions\n",
    "use_simple_loss = False              # if True, the only objective in first pass is to minimize spread of J''\n",
    "iterate_to_find_Jd = True            # False ==> CHEAT in training of GF using ground-truth J' values (currently also requires use_point_transformation = False)\n",
    "use_dropout = True                   # for better regularization\n",
    "write_predictions_on_the_fly = False # do we want to write to fitspred_name_out at each best solution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some astrophysical units and coordinate conversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yr_unit=31.5576e6          # in s\n",
    "Gyr_unit=yr_unit * 1.0e9   # in s\n",
    "AU_unit=149.597870700e6    # in km\n",
    "kpc_unit=3.0857e16         # in km\n",
    "vel_unit=kpc_unit/Gyr_unit # i.e. 1 vel_unit=0.97803 km/s\n",
    "PM_unit=AU_unit/yr_unit    # in km/s (=4.74047)\n",
    "PM_conv_masyr_radGyr=0.20626\n",
    "\n",
    "xr_G=192.85948402*torch.pi/180.0 # Poleski's 2013 alpha_G  NGP in equatorial coordinates\n",
    "xd_G= 27.12829637*torch.pi/180.0 # his delta_G\n",
    "sin_xd_G=np.sin(xd_G)\n",
    "cos_xd_G=np.cos(xd_G)\n",
    "\n",
    "xl_NP=122.93193212*torch.pi/180.0 # North pole in Galactic coordinates\n",
    "xb_NP= 27.12835496*torch.pi/180.0 \n",
    "sin_xb_NP=np.sin(xb_NP)\n",
    "cos_xb_NP=np.cos(xb_NP)\n",
    "\n",
    "RMAT_EqGal = torch.tensor([\n",
    "[-0.054875539726,-0.873437108010,-0.483834985808],\n",
    "[ 0.494109453312,-0.444829589425, 0.746982251810],\n",
    "[-0.867666135858,-0.198076386122, 0.455983795705]],dtype=dtype) # for use in Equatorial to Galactic coordinate conversion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Solar motion, and corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vSun_pec = torch.tensor([11.1, 7.2, 7.25],dtype=dtype) # Schoenrich et al (2010) for U,W; V from Bovy (2020)\n",
    "vc_Sun   = torch.tensor([0.0, 243.0, 0.0],dtype=dtype) # +/- 8 km/s  Bovy (2020)\n",
    "Rsun = 8.224 # +/- 8 km/s      Bovy (2020)\n",
    "zsun = 0.000 # 2.8 pc Widmark et al. (2021), rounded down to zero for mu_l and mu_b to be simple in R and z\n",
    "vSun=vc_Sun+vSun_pec\n",
    "\n",
    "Rsun = Rsun/pos_scale\n",
    "zsun = zsun/pos_scale\n",
    "vc_Sun = vc_Sun/vel_scale\n",
    "vSun_pec = vSun_pec/vel_scale\n",
    "vSun = vSun/vel_scale\n",
    "vc_Sun = vc_Sun/vel_unit # convert into units with G=1\n",
    "vSun_pec = vSun_pec/vel_unit\n",
    "vSun = vSun/vel_unit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct a Dataset class for training\n",
    "## The data is input as a fits file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_as_pandas(fits_flname_read):\n",
    "    tab = fits.open(fits_flname_read)\n",
    "\n",
    "    \n",
    "    labels = tab[2].header\n",
    "    raw_data = tab[2].data\n",
    "        \n",
    "    labels_list = []\n",
    "    for i in range(0,len(labels)):\n",
    "        if 'TTYPE'+str(i) in labels:\n",
    "            labels_list.append(labels['TTYPE'+str(i)])\n",
    "         \n",
    "    table = Table(raw_data,names=labels_list)\n",
    "    nv = len(table)\n",
    "        \n",
    "    orig_datav = table.copy()\n",
    "\n",
    "\n",
    "    labels = tab[3].header\n",
    "    raw_data = tab[3].data\n",
    "        \n",
    "    labels_list = []\n",
    "    for i in range(0,len(labels)):\n",
    "        if 'TTYPE'+str(i) in labels:\n",
    "            labels_list.append(labels['TTYPE'+str(i)])\n",
    "         \n",
    "    table = Table(raw_data,names=labels_list)\n",
    "    np = len(table)\n",
    "        \n",
    "    orig_datap = table.copy()\n",
    "\n",
    "\n",
    "    \n",
    "    labels = tab[1].header\n",
    "    raw_data = tab[1].data\n",
    "        \n",
    "    labels_list = []\n",
    "    for i in range(0,len(labels)):\n",
    "        if 'TTYPE'+str(i) in labels:\n",
    "            labels_list.append(labels['TTYPE'+str(i)])\n",
    "         \n",
    "    table = Table(raw_data,names=labels_list)\n",
    "    n = len(table)\n",
    "        \n",
    "    orig_data = table.copy()\n",
    "    \n",
    "    assert nv == n, \"The fits file must be sensible\"\n",
    "    assert np == n, \"The fits file must be sensible\"\n",
    "    \n",
    "\n",
    "    physical_data = table.copy()\n",
    "    Js_data = table.copy()\n",
    "    thetas_data = table.copy()\n",
    "    dJs_data = table.copy()\n",
    "    dthetas_data = table.copy()\n",
    "\n",
    "\n",
    "    del physical_data[[labels_list[:]][0]]\n",
    "    del Js_data[[labels_list[:]][0]]\n",
    "    del thetas_data[[labels_list[:]][0]]\n",
    "    del dJs_data[[labels_list[:]][0]]\n",
    "    del dthetas_data[[labels_list[:]][0]]\n",
    "\n",
    "\n",
    "    # Expected inputs:\n",
    "    # dis (distance in kpc), xl (Galactic longitude in radians), xb (Galactic latitude in radians)\n",
    "    # vr (Heliocentric radial velocity in km/s),\n",
    "    # ld (mu_l*cos(b), proper motion along Galactic longitude in mas/yr)\n",
    "    # bd (mu_b, proper motion along Galactic latitude in mas/yr)\n",
    "    for i in range(0,NStars_per_stream):\n",
    "        ii = \"%03d\" % (i,)\n",
    "        \n",
    "        colname='dis' + ii\n",
    "        var = Column(orig_data[colname], name = colname)\n",
    "        var = var/pos_scale\n",
    "        physical_data.add_column(var)\n",
    "\n",
    "        colname='xl' + ii\n",
    "        var = Column(orig_data[colname], name = colname)\n",
    "        physical_data.add_column(var)\n",
    "\n",
    "        colname='xb' + ii\n",
    "        var = Column(orig_data[colname], name = colname)\n",
    "        physical_data.add_column(var)\n",
    "\n",
    "        colname='vr' + ii\n",
    "        var = Column(orig_datav[colname], name = colname)\n",
    "        var = var/vel_unit # put into units with G=1\n",
    "        var = var/vel_scale\n",
    "        physical_data.add_column(var)\n",
    "\n",
    "        colname='ld' + ii\n",
    "        var = Column(orig_datav[colname], name = colname)\n",
    "        var = var/PM_conv_masyr_radGyr # put into units with G=1 (radians/Gyr)\n",
    "        var = var/T_scale\n",
    "        physical_data.add_column(var)\n",
    "\n",
    "        colname='bd' + ii\n",
    "        var = Column(orig_datav[colname], name = colname)\n",
    "        var = var/PM_conv_masyr_radGyr # put into units with G=1 (radians/Gyr)\n",
    "        var = var/T_scale\n",
    "        physical_data.add_column(var)\n",
    "\n",
    "\n",
    "    # The reference actions J1,J2,J3 (useful for checks, if not available give dummy values!)\n",
    "    colname='J1'\n",
    "    var = Column(orig_data[colname], name = colname)\n",
    "    var = var/J_scale\n",
    "    Js_data.add_column(var)\n",
    "\n",
    "    colname='J2'\n",
    "    var = Column(orig_data[colname], name = colname)\n",
    "    var = var/J_scale\n",
    "    Js_data.add_column(var)\n",
    "\n",
    "    colname='J3'\n",
    "    var = Column(orig_data[colname], name = colname)\n",
    "    var = var/J_scale\n",
    "    Js_data.add_column(var)\n",
    "\n",
    "    # The reference angles op,oz,or conjugate to J1,J2,J3 (useful for checks, if not available give dummy values!)\n",
    "    for i in range(0,NStars_per_stream):\n",
    "        ii = \"%03d\" % (i,)\n",
    "\n",
    "        colname='op' + ii\n",
    "        var = Column(orig_datap[colname], name = colname)\n",
    "        thetas_data.add_column(var)\n",
    "\n",
    "        colname='oz' + ii\n",
    "        var = Column(orig_datap[colname], name = colname)\n",
    "        thetas_data.add_column(var)\n",
    "\n",
    "        colname='or' + ii\n",
    "        var = Column(orig_datap[colname], name = colname)\n",
    "        thetas_data.add_column(var)\n",
    "\n",
    "\n",
    "    physical = physical_data.to_pandas()\n",
    "    Js       = Js_data.to_pandas()\n",
    "    thetas   = thetas_data.to_pandas()\n",
    "\n",
    "    tab.close()\n",
    "    \n",
    "    return physical, Js, thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import dataset\n",
    "\n",
    "from astropy.table import Column\n",
    "import astropy.io.fits as fits \n",
    "from astropy.table import Table\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class AstroDataset(dataset.Dataset):\n",
    "    \"\"\"\n",
    "    Class for feeding data within a DataLoader object.     \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, fitsfile = 'dummy.fits',   mode='train', randomize=True ):\n",
    "        super().__init__()\n",
    "        \n",
    "    \n",
    "        df, df_ref, dfo_ref = get_data_as_pandas(fitsfile)\n",
    "        self.randomize = randomize\n",
    "        \n",
    "        assert len(df) >= ndata_select,      \"ndata_select      must not be larger than the dataset\"\n",
    "        assert len(df) >= ndata_select_test, \"ndata_select_test must not be larger than the dataset\"\n",
    "        \n",
    "        train_df = df[:ndata_select]\n",
    "        test_df  = df[len(df)-ndata_select_test:]\n",
    "\n",
    "        train_df_ref = df_ref[:ndata_select]\n",
    "        test_df_ref  = df_ref[len(df)-ndata_select_test:]\n",
    "\n",
    "        train_dfo_ref = dfo_ref[:ndata_select]\n",
    "        test_dfo_ref  = dfo_ref[len(df)-ndata_select_test:]\n",
    "\n",
    "\n",
    "        if mode =='train':\n",
    "            nlen=min(len(train_df),ndata_select)\n",
    "            self.df = train_df[:nlen]\n",
    "            self.df_ref = train_df_ref[:nlen]\n",
    "            self.dfo_ref = train_dfo_ref[:nlen]\n",
    "        elif mode =='validation':\n",
    "            nlen=min(len(test_df),ndata_select_test)\n",
    "            self.df = test_df[:nlen]\n",
    "            self.df_ref = test_df_ref[:nlen]\n",
    "            self.dfo_ref = test_dfo_ref[:nlen]\n",
    "        elif mode =='all':\n",
    "            self.df = df\n",
    "            self.df_ref = df_ref\n",
    "            self.dfo_ref = dfo_ref\n",
    "        else:\n",
    "            raise ValueError(\"Cannot understand mode for training, available choices::{train,validation,all}, aborting ...\")\n",
    "    \n",
    "        self.mode = mode\n",
    "       \n",
    "\n",
    "        nelem=len(self.df)\n",
    "    \n",
    "        self.variables_arr        = np.zeros((nelem,NStars_per_stream*NPhase),dtype=np_dtype)\n",
    "        self.variables_arr[:,:]   = self.df.iloc[:,:].astype(np_dtype)\n",
    "        self.variables_arr        = np.reshape(self.variables_arr,(nelem,NStars_per_stream,NPhase))\n",
    "        \n",
    "        self.Js_ref_arr           = np.zeros((nelem,1*NPhase//2),dtype=np_dtype)\n",
    "        self.Js_ref_arr[:,:]      = self.df_ref.iloc[:,:].values.astype(np_dtype)\n",
    "        self.Js_ref_arr           = np.reshape(self.Js_ref_arr,(nelem,1,NPhase//2))\n",
    "\n",
    "        self.thetas_ref_arr       = np.zeros((nelem,NStars_per_stream*NPhase//2),dtype=np_dtype)\n",
    "        self.thetas_ref_arr[:,:]  = self.dfo_ref.iloc[:,:].values.astype(np_dtype)\n",
    "        self.thetas_ref_arr       = np.reshape(self.thetas_ref_arr,(nelem,NStars_per_stream,NPhase//2))\n",
    "        self.thetas_ref_arr       = ( (self.thetas_ref_arr) % (2 * np.pi) ) # i.e. [0,2*pi]\n",
    "\n",
    "\n",
    "        dis    = np.cos(self.variables_arr[:,:,0:1])\n",
    "        cos_xl = np.cos(self.variables_arr[:,:,1:2])\n",
    "        sin_xl = np.sin(self.variables_arr[:,:,1:2])\n",
    "        cos_xb = np.cos(self.variables_arr[:,:,2:3])\n",
    "        sin_xb = np.sin(self.variables_arr[:,:,2:3])\n",
    "        \n",
    "        rvec=np.concatenate(( cos_xl*cos_xb, sin_xl*cos_xb, sin_xb),axis=2)\n",
    "        lvec=np.concatenate((-sin_xl       , cos_xl       , np.zeros_like(cos_xl)),axis=2)\n",
    "        bvec=np.concatenate((-cos_xl*sin_xb,-sin_xl*sin_xb, cos_xb),axis=2)\n",
    "        \n",
    "        vr_vcorr = np.einsum('bci,i->bc',rvec,vSun)[:,:,None]\n",
    "        vl_vcorr = np.einsum('bci,i->bc',lvec,vSun)[:,:,None]\n",
    "        vb_vcorr = np.einsum('bci,i->bc',bvec,vSun)[:,:,None]\n",
    "\n",
    "        self.vcorr_arr=np.concatenate((vr_vcorr,vl_vcorr,vb_vcorr,vr_vcorr/dis,vl_vcorr/dis,vb_vcorr/dis),axis=2)\n",
    "\n",
    "        # remove the Solar motion first.\n",
    "        self.variables_arr[:,:,3:4]=self.variables_arr[:,:,3:4]+self.vcorr_arr[:,:,0:1]\n",
    "        self.variables_arr[:,:,4:5]=self.variables_arr[:,:,4:5]+self.vcorr_arr[:,:,1:2]/self.variables_arr[:,:,0:1]\n",
    "        self.variables_arr[:,:,5:6]=self.variables_arr[:,:,5:6]+self.vcorr_arr[:,:,2:3]/self.variables_arr[:,:,0:1]\n",
    "        \n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        \"\"\"\n",
    "        This function returns the the input to the network, and ground truth pairs for a particular index. \n",
    "        \"\"\"\n",
    "        \n",
    "        pick_timestep=np.random.permutation(NStars_per_stream) # NOTE the random permutation of the entries\n",
    "\n",
    "        \n",
    "        variables  = self.variables_arr[idx,pick_timestep[0:NStars],:]\n",
    "        Js_ref     = self.Js_ref_arr[idx,0:1,:]\n",
    "        thetas_ref = self.thetas_ref_arr[idx,pick_timestep[0:NStars],:]\n",
    "        vcorr_arr  = self.vcorr_arr[idx,pick_timestep[0:NStars],:]\n",
    "\n",
    "        return variables, Js_ref, thetas_ref\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        This function returns the length of the dataset, total elements. \n",
    "        \"\"\"\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the data visually to make sure all is OK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_check  = AstroDataset(fitsfile=fitsdata_name,mode='all')\n",
    "variables = dataset_check.df.iloc[0,:]\n",
    "\n",
    "print(\"Length of full dataset:\",len(dataset_check))\n",
    "\n",
    "dataset_check  = AstroDataset(fitsfile=fitsdata_name,mode='validation')\n",
    "variables = dataset_check.df.iloc[0,:]\n",
    "\n",
    "print(\"Length of validation dataset:\",len(dataset_check))\n",
    "\n",
    "dataset_check  = AstroDataset(fitsfile=fitsdata_name,mode='train')\n",
    "variables = dataset_check.df.iloc[0,:]\n",
    "\n",
    "print(\"Length of training dataset:\",len(dataset_check))\n",
    "\n",
    "\n",
    "for i in range(0,1):\n",
    "    net_input, Js_ref, thetas_ref = dataset_check[10]\n",
    "\n",
    "net_input.shape, thetas_ref.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class ResDense_block(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Basic building block, using the philosophy of resnets but for Linear blocks.\n",
    "    This layer does not change the number of features. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, nunits, minwidth_forDropout=256, pDropout=0.5):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        We DECLARE the layers we want to use. These are the elements that will go into the nodes of \n",
    "        the computational graph. The graph (i.e. how the layers are connected) is defined in the forward function. \n",
    "        \"\"\"\n",
    "        \n",
    "        self.nunits = nunits\n",
    "        self.minwidth_forDropout = minwidth_forDropout\n",
    "            \n",
    "        self.DO1    = torch.nn.Dropout(p=pDropout)\n",
    "        self.dense1 = torch.nn.Linear(in_features=self.nunits, out_features= self.nunits, bias=False)\n",
    "        self.WN1    = torch.nn.utils.weight_norm(self.dense1)\n",
    "        self.DO2    = torch.nn.Dropout(p=pDropout)\n",
    "        self.dense2 = torch.nn.Linear(in_features=self.nunits, out_features= self.nunits)\n",
    "        self.WN2    = torch.nn.utils.weight_norm(self.dense2)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        This function is the definition of the computational graph, i.e. the connection of inputs to layers, and layers interconnection.  \n",
    "        \"\"\"\n",
    "        \n",
    "        xx = torch.relu(input)\n",
    "        if (use_dropout and self.nunits>=self.minwidth_forDropout):\n",
    "            xx = self.DO1(xx)\n",
    "        xx = self.dense1(xx)\n",
    "        xx = self.WN1(xx)\n",
    "\n",
    "        xx = torch.relu(xx)\n",
    "        if (use_dropout and self.nunits>=self.minwidth_forDropout):\n",
    "            xx = self.DO2(xx)\n",
    "        xx = self.dense2(xx)\n",
    "        xx = self.WN2(xx)\n",
    "\n",
    "        xx = xx+input\n",
    "            \n",
    "        return xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class CapNet(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Rodrigo's lobotomization of Foivos' fancy CapNet\n",
    "    \"\"\"\n",
    "    def __init__(self, depth, in_features=NPhase//2, out_features=1, st_features=64, verbose=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.depth = depth\n",
    "        width_max=(st_features)*2**4\n",
    "        \n",
    "        assert width_max >= out_features, \"width_max too small\"\n",
    "        assert st_features >= in_features, \"st_features too small\"\n",
    "        assert st_features >= out_features, \"st_features too small\"\n",
    "        \n",
    "        verbose=True\n",
    "        if (verbose):\n",
    "            print (\"***** CapNet structure *****\")\n",
    "\n",
    "        encoder = []\n",
    "\n",
    "        encoder.append(torch.nn.Linear(in_features=in_features, out_features=st_features))\n",
    "\n",
    "        for i in range(0,depth):\n",
    "            if (verbose):\n",
    "                print (\"depth:= {0}, width: {1}, target: {2}\".format(i,min(st_features*2**i,width_max),out_features))\n",
    "            encoder.append(ResDense_block(nunits=min(st_features*2**i,width_max)))\n",
    "            if (min(st_features*2**i,width_max) != min(st_features*2**(i+1),width_max)):\n",
    "                encoder.append(torch.nn.Linear(in_features=min(st_features*2**i,width_max), \n",
    "                                            out_features=min(st_features*2**(i+1),width_max)))\n",
    "\n",
    "        current_features=min(st_features*2**depth,width_max)\n",
    "\n",
    "        if (verbose):\n",
    "            print (\"depth:= {0}, width: {1}, target: {2}\".format(depth,current_features,out_features))\n",
    "        encoder.append(ResDense_block(nunits=current_features))\n",
    "\n",
    "        assert current_features >= out_features, \"depth too small\"\n",
    "\n",
    "        \n",
    "        for i in range(depth-1,-1,-1):\n",
    "            if (min(st_features*2**i,width_max)>=out_features):\n",
    "                current_features=min(st_features*2**i,width_max)\n",
    "\n",
    "                assert min(st_features*2**(i+1),width_max) >= current_features, \"something weird\"\n",
    "                \n",
    "                if (min(st_features*2**(i+1),width_max) != current_features):\n",
    "                    encoder.append(torch.nn.Linear(in_features=min(st_features*2**(i+1),width_max), \n",
    "                                           out_features=current_features))\n",
    "                if (verbose):\n",
    "                    print (\"depth:= {0}, width: {1}, target: {2}\".format(i,current_features,out_features))\n",
    "                encoder.append(ResDense_block(nunits=current_features))\n",
    "\n",
    "        \n",
    "        encoder.append(torch.nn.Linear(in_features=current_features, out_features=out_features))\n",
    "        self.encoder = torch.nn.Sequential(*encoder)\n",
    "\n",
    "        \n",
    "    def forward(self, input ):\n",
    "            \n",
    "        x = self.encoder(input)\n",
    "                \n",
    "        return x    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The analytic toy model\n",
    "## output order is J_phi, J_z, J_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Isochrone_Analytic(torch.nn.Module):\n",
    "    def __init__(self, verbose=False, tiny=1.0e-10):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tiny = tiny\n",
    "\n",
    "    def forward(self,SetOfStars,M_iso,b_iso):\n",
    "        # largely a pytorch translation (with some cleaning to get back-propagation to work) of J. Bovy's actionAngleIsochrone.py \n",
    "        # Given its location in the network, it would pay off to spend some effort rationalizing this routine !!!\n",
    "        \n",
    "        x=SetOfStars[...,0:1]\n",
    "        y=SetOfStars[...,1:2]\n",
    "        z=SetOfStars[...,2:3]\n",
    "        vx=SetOfStars[...,3:4]\n",
    "        vy=SetOfStars[...,4:5]\n",
    "        vz=SetOfStars[...,5:6]\n",
    "\n",
    "        r2 = x**2+y**2+z**2\n",
    "        r  = torch.sqrt(r2)\n",
    "        r_soft = torch.sqrt(b_iso**2+r2)\n",
    "        R2 = x**2+y**2\n",
    "        R  = torch.sqrt(R2)\n",
    "        phi= torch.atan2(y,x)\n",
    "        v2 = vx**2+vy**2+vz**2\n",
    "        v  = torch.sqrt(v2)\n",
    "        vR = (x*vx+y*vy)/R\n",
    "\n",
    "        Lx = y*vz-z*vy\n",
    "        Ly = z*vx-x*vz\n",
    "        Lz = x*vy-y*vx\n",
    "    \n",
    "        L2 = Lx**2+Ly**2+Lz**2\n",
    "        L  = torch.sqrt(L2)\n",
    "        \n",
    "        pot= -M_iso/(b_iso+r_soft)\n",
    "        E  = pot+0.5*v2\n",
    "        E[E>-self.tiny]=-self.tiny  # NOTE!!! energy of unbound orbits is fudged here to avoid numerical problems below\n",
    "        \n",
    "        J2 = L - torch.abs(Lz)\n",
    "        J3 = M_iso/torch.sqrt(-2.0*E)-0.5*(L+torch.sqrt(L2+4*M_iso*b_iso))\n",
    "        J3[torch.isnan(J3)]=self.tiny\n",
    "\n",
    "  \n",
    "        #Frequencies\n",
    "        Omegar= (-2.0*E)**1.5/M_iso\n",
    "        Omegar[torch.isnan(Omegar)]=0.0\n",
    "        Omegaz= 0.5*(1.0+L/torch.sqrt(L2+4.0*M_iso*b_iso))*Omegar\n",
    "        Omegap= Omegaz.clone()\n",
    "        \n",
    "        indx= (Lz < 0.0)\n",
    "        Omegap[indx] *= -1.0\n",
    "\n",
    "        #Angles\n",
    "        c  = -M_iso/2.0/E-b_iso\n",
    "        e  = torch.sqrt( 1.0 - L2/M_iso/c*(1.0+b_iso/c) )\n",
    "\n",
    "        if b_iso == 0.0:\n",
    "            coseta = 1.0/e*(1.0-torch.sqrt(r2)/c)\n",
    "        else:\n",
    "            s= 1.0+torch.sqrt(1.0+r2/b_iso**2)\n",
    "            coseta= 1.0/e*(1.0-b_iso/c*(s-2.0))\n",
    "\n",
    "        torch.clamp(coseta, min=-1.0,max=1.0)\n",
    "        eta= torch.acos(coseta)\n",
    "        costheta= z/r\n",
    "        sintheta= R/r\n",
    "        vrindx= ((vR*sintheta+vz*costheta) < 0.0)\n",
    "        eta[vrindx] = 2.0*torch.pi-eta[vrindx]\n",
    "        angler= eta-e*c/(c+b_iso)*torch.sin(eta)\n",
    "        tan11= torch.atan(torch.sqrt((1.0+e)/(1.0-e))*torch.tan(0.5*eta))\n",
    "        tan12= torch.atan(torch.sqrt((1.0+e+2.0*b_iso/c)/(1.0-e+2.0*b_iso/c))*torch.tan(0.5*eta))\n",
    "\n",
    "        vzindx= ((-vz*sintheta+vR*costheta) > 0.0)\n",
    "\n",
    "        tan11[tan11 < 0.0] += torch.pi\n",
    "        tan12[tan12 < 0.0] += torch.pi\n",
    "\n",
    "        pindx1= (Lz/L >  1.0)\n",
    "        pindx2= (Lz/L < -1.0)\n",
    "        Lz_clamped = Lz.clone()\n",
    "        Lz_clamped[pindx1]=  L[pindx1]\n",
    "        Lz_clamped[pindx2]= -L[pindx2]\n",
    "        \n",
    "        sini= torch.sqrt(L2-Lz_clamped**2)/L\n",
    "        tani= torch.sqrt(L2-Lz_clamped**2)/Lz_clamped\n",
    "        sinpsi= costheta/sini\n",
    "    \n",
    "        pindx1= (sinpsi > 1.0)*torch.isfinite(sinpsi)\n",
    "        sinpsi[pindx1]= 1.0\n",
    "        pindx2= (sinpsi < -1.0)*torch.isfinite(sinpsi)\n",
    "        sinpsi[pindx2]= -1.0           \n",
    "    \n",
    "        psi= torch.asin(sinpsi)\n",
    "        psi[vzindx]= torch.pi-psi[vzindx]\n",
    "    \n",
    "        # For non-inclined orbits, set Omega=0 by convention\n",
    "        psi[~torch.isfinite(psi)] = phi[~torch.isfinite(psi)]\n",
    "        psi= psi % (2.0*torch.pi)\n",
    "        anglez= psi+Omegaz/Omegar*angler-tan11-1.0/torch.sqrt(1.0+4*M_iso*b_iso/L2)*tan12\n",
    "        sinu= z/R/tani\n",
    "    \n",
    "        pindx3= (sinu > 1.0)*torch.isfinite(sinu)\n",
    "        sinu[pindx3]= 1.0\n",
    "        pindx4= (sinu < -1.0)*torch.isfinite(sinu)\n",
    "        sinu[pindx4]= -1.0           \n",
    "\n",
    "        u= torch.asin(sinu)\n",
    "        u[vzindx]= torch.pi-u[vzindx]\n",
    "\n",
    "        # For non-inclined orbits, set Omega=0 by convention\n",
    "        u[~torch.isfinite(u)]= phi[~torch.isfinite(u)]\n",
    "        Omega = phi-u\n",
    "        anglep= Omega.clone()\n",
    "    \n",
    "        anglep[indx]  -= anglez[indx]\n",
    "        anglep[~indx] += anglez[~indx]\n",
    "\n",
    "        indxp = torch.isnan(anglep)\n",
    "        indxz = torch.isnan(anglez)\n",
    "        indxr = torch.isnan(angler)\n",
    "\n",
    "        Anglep= anglep % (2*torch.pi)\n",
    "        Anglez= anglez % (2*torch.pi)\n",
    "        Angler= angler % (2*torch.pi)\n",
    "\n",
    "        Anglep[indxp] = 0.\n",
    "        Anglez[indxz] = 0.\n",
    "        Angler[indxr] = 0.\n",
    "        \n",
    "        acc_fac = M_iso/( r_soft * (b_iso+r_soft)**2 )\n",
    "        ax = -x*acc_fac\n",
    "        ay = -y*acc_fac\n",
    "        az = -z*acc_fac        \n",
    "        \n",
    "        return (torch.cat([Lz,J2,J3],dim=-1),\n",
    "                torch.cat([Anglep,Anglez,Angler],dim=-1),\n",
    "                torch.cat([Omegap,Omegaz,Omegar],dim=-1),\n",
    "                torch.cat([ax,ay,az],dim=-1) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility to convert: distance,l,b,vr,mu_l,mu_b -> x,y,z,vx,vy,vz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_Input2xv(torch.nn.Module):\n",
    "    def __init__(self, verbose=False):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self,SetOfStars):\n",
    "        \n",
    "        dis           = SetOfStars[...,0:1]\n",
    "        xl            = SetOfStars[...,1:2]\n",
    "        xb            = SetOfStars[...,2:3]\n",
    "        vr            = SetOfStars[...,3:4]\n",
    "        ldot_CB_vcorr = SetOfStars[...,4:5]\n",
    "        bdot_vcorr    = SetOfStars[...,5:6]\n",
    "        \n",
    "        cos_xl = torch.cos(xl)\n",
    "        sin_xl = torch.sin(xl)\n",
    "        cos_xb = torch.cos(xb)\n",
    "        sin_xb = torch.sin(xb)\n",
    "        \n",
    "        rvec=torch.cat(( cos_xl*cos_xb, sin_xl*cos_xb, sin_xb),axis=2)\n",
    "        lvec=torch.cat((-sin_xl       , cos_xl       , torch.zeros_like(cos_xl)),axis=2)\n",
    "        bvec=torch.cat((-cos_xl*sin_xb,-sin_xl*sin_xb, cos_xb),axis=2)\n",
    "        rlbRMAT=torch.stack([rvec,lvec,bvec],dim=-1)\n",
    "        \n",
    "        vl = ldot_CB_vcorr*dis\n",
    "        vb = bdot_vcorr*dis\n",
    "        vec_vcorr=torch.cat([vr,vl,vb],dim=-1)\n",
    "\n",
    "        x=dis*rlbRMAT[:,:,0:1,0]-Rsun         # the position vector from the GC\n",
    "        y=dis*rlbRMAT[:,:,1:2,0]\n",
    "        z=dis*rlbRMAT[:,:,2:3,0]+zsun\n",
    "\n",
    "        vx=torch.einsum('bci,bci->bc',vec_vcorr,rlbRMAT[:,:,0,:])[:,:,None]\n",
    "        vy=torch.einsum('bci,bci->bc',vec_vcorr,rlbRMAT[:,:,1,:])[:,:,None]\n",
    "        vz=torch.einsum('bci,bci->bc',vec_vcorr,rlbRMAT[:,:,2,:])[:,:,None]\n",
    "\n",
    "        SetOfStars_xv=torch.cat([x,y,z,vx,vy,vz],dim=2)\n",
    "            \n",
    "        return SetOfStars_xv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nice and fast Jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacobian(inputs, outputs,create_graph=True,retain_graph=True):\n",
    "    # This calculates gradients for each J coordinate - index i\n",
    "    return  torch.stack([ torch.autograd.grad(outputs[:,:,i].sum(), inputs, \n",
    "                                    retain_graph=retain_graph, \n",
    "                                    create_graph=create_graph, \n",
    "                                    only_inputs=True)[0] \n",
    "                        for i in range(outputs.shape[-1])], dim=2) \n",
    "# (it might be possible to make this completely general by using pytorch's \"...\" notation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next come the encoders for:\n",
    "### Encoder_GF_G: canonical generating function G\n",
    "### Encoder_GF_P: canonical point-transformation P\n",
    "### Encoder_A   : the acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_A(torch.nn.Module):\n",
    "    def __init__(self, verbose=False):\n",
    "        super().__init__()\n",
    "        self.ultra  = CapNet(depth=depth_A, in_features=NPhase, out_features=NPhase//2)\n",
    "\n",
    "    def forward(self,SetOfStars_wacc):\n",
    "\n",
    "        acc_iso = SetOfStars_wacc[...,3:]\n",
    "        \n",
    "        acc = acc_iso + self.ultra(SetOfStars_wacc)\n",
    "        \n",
    "        return acc\n",
    "\n",
    "\n",
    "class Encoder_GF_P(torch.nn.Module):\n",
    "    def __init__(self, verbose=False):\n",
    "        super().__init__()\n",
    "        self.ultra  = CapNet(depth=depth_GF_P, in_features=NPhase, out_features=NPhase)\n",
    "\n",
    "    def forward(self,SetOfStars_T):\n",
    "        \n",
    "        SetOfStars = torch.cat([torch.cos(SetOfStars_T),torch.sin(SetOfStars_T)],dim=-1)\n",
    "\n",
    "        Tout = self.ultra(SetOfStars)\n",
    "        \n",
    "        GF_P = (SetOfStars_T + torch.atan2(Tout[...,:3],Tout[...,3:])) % (2*torch.pi)\n",
    "        \n",
    "        return GF_P\n",
    "\n",
    "\n",
    "class Encoder_GF_G(torch.nn.Module):\n",
    "    def __init__(self, verbose=False):\n",
    "        super().__init__()\n",
    "        self.ultra = CapNet(depth=depth_GF_G, in_features=NPhase+NPhase//2, out_features=3)\n",
    "\n",
    "    def forward(self,SetOfStars_TJ):\n",
    "\n",
    "        SetOfStars_Ts = SetOfStars_TJ[...,:3]\n",
    "        SetOfStars_Js = SetOfStars_TJ[...,3:]\n",
    "        SetOfStars = torch.cat([torch.cos(SetOfStars_Ts),torch.sin(SetOfStars_Ts),SetOfStars_Js],dim=-1)\n",
    "        \n",
    "        GFU = self.ultra(SetOfStars)\n",
    "\n",
    "        GF_G = ( torch.atan2(GFU[...,1:2],GFU[...,0:1]) + GFU[...,2:3] )\n",
    "        \n",
    "        return GF_G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main ActionFinder network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionFinder(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Inputs: \n",
    "           d,l,b,vh,mu_l,mu_b astrometric phase space coordinates\n",
    "    Output: \n",
    "           loss due to the spread of the J'' values (and others)\n",
    "    \"\"\"\n",
    "    def __init__(self,verbose=False,iter_max=10):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_input2xv = conv_Input2xv(verbose=verbose)\n",
    "        self.isochrone_analytic = Isochrone_Analytic(verbose=verbose)\n",
    "        self.encoder_GF_G = Encoder_GF_G(verbose=verbose)\n",
    "        self.encoder_GF_P = Encoder_GF_P(verbose=verbose)\n",
    "        self.encoder_A    = Encoder_A(verbose=verbose)\n",
    "        self.Mb_param     = torch.nn.Parameter(torch.ones((2,1),dtype=torch.float64))\n",
    "\n",
    "        self.iter_max = iter_max\n",
    "        \n",
    "    def forward(self,inputs,Jlabels,Tlabels,i,epoch): # Note that we're feeding in the J,theta labels, but purely for debugging. The algorithm can work completely unsupervised.\n",
    "    \n",
    "        M_iso_toy  = np.abs(M_iso_ref)*self.Mb_param[0] # mass and scale of the toy isochrone model\n",
    "        b_iso_toy  = np.abs(b_iso_ref)*self.Mb_param[1] # Mb_param[] is optimized by the network\n",
    "                    \n",
    "        # ***NOTE*** double dashed variables (Tdd, Jdd) i.e. (Theta'', J'') are in the ***target system***\n",
    "        #                   dashed variables (Td,  Jd)  i.e. (Theta', J')   result from applying G transformations\n",
    "\n",
    "        inputs_xv = self.conv_input2xv(inputs)        # convert d,l,b,vh,mu_l,mu_b to xv\n",
    "        \n",
    "        J_iso,  T_iso,  O_iso,  a_iso = self.isochrone_analytic(inputs_xv, M_iso_toy,b_iso_toy)  # (J,Theta, freq & acceleration) of isochrone toy model \n",
    "\n",
    "        if (Fit_Acceleration):\n",
    "            inputs_x = inputs_xv[...,:3]\n",
    "            Accxv_model = self.encoder_A( torch.cat([inputs_x,a_iso],dim=-1) )\n",
    "\n",
    "        \n",
    "        if (iterate_to_find_Jd):\n",
    "            # We now iterate to find J'. Start with J' = mean(J_iso for each stream)\n",
    "            Jd_mean = torch.mean( J_iso, dim=1, keepdim=True)\n",
    "\n",
    "            for iter in range(self.iter_max):\n",
    "                Jd_trial = Jd_mean.clone()                # trial J'\n",
    "                Jd_mean_fill = torch.cat([Jd_mean]*NStars,dim=1)\n",
    "\n",
    "                TJd  = torch.cat( [T_iso,Jd_mean_fill],dim=-1 ) # (T,J')\n",
    "                GF_G = self.encoder_GF_G(TJd)\n",
    "                JmJd = jacobian(T_iso,GF_G)[:,:,0,:].data # remove from computational graph\n",
    "                GF_G.detach()                             # remove from computational graph\n",
    "\n",
    "                # update \n",
    "                Jd_mean = torch.mean( J_iso - JmJd, dim=1, keepdim=True)\n",
    "\n",
    "                if ( torch.max( torch.abs(Jd_trial - Jd_mean)) < 5.e-5 and \n",
    "                     torch.mean(torch.abs(Jd_trial - Jd_mean)) < 1.e-6):\n",
    "                    break\n",
    "        else:\n",
    "            # CHEAT by initializing with ground-truth values!!! (useful for exploring what the GF is capable of)\n",
    "            Jd_mean = Jlabels.clone() # currently, for this to work, also set use_point_transformation=False\n",
    "                \n",
    "                \n",
    "        # Find Jd_mean once more, but now retain the computational graph \n",
    "        Jd_mean_fill = torch.cat([Jd_mean]*NStars,dim=1)\n",
    "        TJd  = torch.cat( [T_iso,Jd_mean_fill],dim=-1 )\n",
    "        GF_G = self.encoder_GF_G(TJd)\n",
    "        JmJd = jacobian(T_iso,GF_G)[:,:,0,:]\n",
    "        Jd_vals = J_iso - JmJd                       # J' given by generating function G\n",
    "        Jd_mean = torch.mean( Jd_vals, dim=1, keepdim=True)\n",
    "                \n",
    "        # Calculate T' from GF derivative\n",
    "        Jd_mean_fill = torch.cat([Jd_mean]*NStars,dim=1)\n",
    "        TJd  = torch.cat( [T_iso,Jd_mean_fill],dim=-1 )\n",
    "        GF_G = self.encoder_GF_G(TJd)\n",
    "\n",
    "        # T' = T + d(G)/d(J'):\n",
    "        Td_vals = (T_iso + jacobian(Jd_mean_fill,GF_G)[:,:,0,:]) % (2*torch.pi)\n",
    "\n",
    "        \n",
    "        if (use_point_transformation):\n",
    "            # Add in the extra freedom of a point-transformation\n",
    "            Tdd_vals = self.encoder_GF_P( Td_vals )      # final T'' from generating function P\n",
    "            dTdd_dTd = jacobian(Td_vals,Tdd_vals)        # d(T'')/d(T')\n",
    "            dTd_dTdd = torch.inverse( dTdd_dTd )         # d(T')/d(T'')\n",
    "            # J_i'' = dT'/dT_i'' . J'\n",
    "            Jdd_vals = torch.einsum('bsji,bsj->bsi',dTd_dTdd,Jd_vals) # final J'' values\n",
    "        else:\n",
    "            Tdd_vals = Td_vals.clone()\n",
    "            Jdd_vals = Jd_vals.clone()\n",
    "\n",
    "        \n",
    "        Jdd_mean = torch.mean( Jdd_vals, dim=1)\n",
    "                \n",
    "        Jdd_spread = torch.mean(torch.abs(Jdd_mean[:,None,:] - Jdd_vals), dim=1)\n",
    "        Jdd_diff   = Jdd_mean[:,None,:] - Jlabels\n",
    "\n",
    "        Tdd_diff   = (Tdd_vals-Tlabels + torch.pi) % (2*torch.pi) - torch.pi # [-pi,pi]\n",
    "        T_diff_iso = (T_iso-Tlabels + torch.pi) % (2*torch.pi) - torch.pi\n",
    "        \n",
    "        \n",
    "        # create Jdd_vals_pos, which we will use later to penalise unphysical -ve J2,J3:\n",
    "        mask_neg = [torch.cat([torch.abs(Jdd_vals[...,0:1]),Jdd_vals[...,1:3]],dim=-1)<0]\n",
    "        Jdd_vals_pos = Jdd_vals.clone()\n",
    "        Jdd_vals_pos[mask_neg] = torch.abs(Jdd_vals[mask_neg])\n",
    "\n",
    "        \n",
    "        # T'=0 should correspond to T=0\n",
    "        TJd0 = torch.cat( [torch.zeros_like(T_iso),Jd_vals],dim=-1 )\n",
    "        GF_G = self.encoder_GF_G(TJd0)\n",
    "        Td_vals0  = (jacobian(Jd_vals,GF_G)[:,:,0,:]) % (2*torch.pi)\n",
    "        if (use_point_transformation):\n",
    "            Tdd_vals0 = (self.encoder_GF_P( Td_vals0 ) + torch.pi) % (2*torch.pi) - torch.pi # [-pi,pi] network for T' -> T''\n",
    "        else:\n",
    "            Tdd_vals0 = Td_vals0 - torch.pi # [-pi,pi]\n",
    "    \n",
    "        \n",
    "        if (Fit_Acceleration):\n",
    "            # Jacobian of the new coordinates wrt xv:\n",
    "            dJdW = jacobian(inputs_xv, Jdd_vals)\n",
    "            dTdW = jacobian(inputs_xv, Tdd_vals)\n",
    "            dQdW = torch.cat([dTdW,dJdW],dim=2)\n",
    "        \n",
    "            gradrJ = dJdW[:,:,:,:3] # grad_r of J\n",
    "            gradvJ = dJdW[:,:,:,3:] # grad_v of J\n",
    "            mask = torch.ByteTensor(batch_size, NStars).zero_().type(torch.bool).to(device)\n",
    "            mask += True\n",
    "            mask_indx = torch.min(torch.abs( gradvJ.det() ),1)[1]\n",
    "            mask[np.arange(batch_size),mask_indx]=0\n",
    "\n",
    "            gradrJ_dot_v = torch.einsum('bsij,bsj->bsi',gradrJ,inputs_xv[:,:,3:])\n",
    "            gradvJ_dot_a = torch.einsum('bsij,bsj->bsi',gradvJ,Accxv_model[:,:,:])\n",
    "            dJdt = gradrJ_dot_v + gradvJ_dot_a  # total derivative of J\n",
    "\n",
    "            gradrPhi = torch.solve(gradrJ_dot_v[:,:,:,None], gradvJ)[0]\n",
    "            Accxv_LinAl = -gradrPhi[:,:,:,0] # the linear algebra solution for the acceleration \n",
    "\n",
    "            gradrT_dot_v = torch.einsum('bsij,bsj->bsi',dTdW[:,:,:,:3],inputs_xv[:,:,3:])\n",
    "            gradvT_dot_a = torch.einsum('bsij,bsj->bsi',dTdW[:,:,:,3:],Accxv_model[:,:,:])\n",
    "            Omegas = gradrT_dot_v + gradvT_dot_a # Omegas == dtheta/dt\n",
    "\n",
    "            Omegas_mean = torch.mean( Omegas, dim=1)\n",
    "            Omegas_spread = torch.mean(torch.abs(Omegas_mean[:,None,:] - Omegas), dim=1) # order consistent with Jdd_spread above\n",
    "\n",
    "            \n",
    "\n",
    "        loss_Jdd = torch.mean( torch.abs( Jdd_diff ) )\n",
    "        loss_Jdd_spread = torch.mean( Jdd_spread )\n",
    "        loss_Jdd_neg = torch.mean( torch.abs( Jdd_vals-Jdd_vals_pos ) )\n",
    "        loss_J_iso = torch.mean( torch.abs( torch.mean( J_iso, dim=1, keepdim=True)-Jlabels ) )\n",
    "    \n",
    "        loss_Tdd = torch.mean( torch.abs( Tdd_diff ) )/(scale_Ts)\n",
    "        loss_Tdd_vals0 = torch.mean(torch.abs( Tdd_vals0 ))/(scale_Ts)\n",
    "        loss_T_iso = torch.mean( torch.abs( T_diff_iso ))/(scale_Ts)\n",
    "        \n",
    "        \n",
    "        if (Fit_Acceleration):\n",
    "            loss_dJdt = torch.mean( torch.abs( dJdt ) )/(scale_dJdt)\n",
    "            loss_Os_spread = torch.mean( Omegas_spread )/(scale_Os_same)\n",
    "            loss_direc = torch.mean( 1 - cossimilarity(-Accxv_model[:,:,0:2],inputs_xv[:,:,0:2]) )/(scale_direc) # cylindrical symmetry\n",
    "            loss_LinAl = torch.mean(torch.abs( (Accxv_model-Accxv_LinAl)[mask] ) )/(scale_LinAl)\n",
    "        else:\n",
    "            loss_dJdt = torch.zeros(1)\n",
    "            loss_Os_spread = torch.zeros(1)\n",
    "            loss_direc = torch.zeros(1)\n",
    "            loss_LinAl = torch.zeros(1)\n",
    "        \n",
    "        \n",
    "        if (i==0): # print out the first point in the (randomly shuffled) list, to check all is ok\n",
    "            for k in range(0,1): \n",
    "\n",
    "                torch.set_printoptions(sci_mode=False,linewidth=120)\n",
    "                print(\" \")\n",
    "                print(\" *********** \")\n",
    "                print(\" \")\n",
    "                print(\"Jdd_vals     :\",(Jdd_vals[k,0,:]).data)\n",
    "                print(\"Jlabels      :\",(Jlabels[k,0,:]).data)\n",
    "                print(\"Jdd_diff     :\",(Jdd_diff[k,0,:]).data)\n",
    "                print(\"Jdd_spread   :\",(Jdd_spread[k,:]).data)\n",
    "                print(\" \")\n",
    "                print(\"Tdd_diff     :\",(Tdd_diff[k,0,:]).data)\n",
    "                print(\" \")\n",
    "                print(\"loss_Jdd_neg :\",loss_Jdd_neg)\n",
    "                print(\" \")\n",
    "                print(\"Isochrone M,b:\",self.Mb_param.data)\n",
    "\n",
    "                torch.set_printoptions(sci_mode=None,linewidth=None)\n",
    "\n",
    "            \n",
    "        if (Fit_Acceleration):\n",
    "            loss = ( loss_Jdd_spread + loss_Jdd_neg + loss_Tdd_vals0 +\n",
    "                     loss_dJdt + loss_Os_spread + loss_direc + loss_LinAl ) \n",
    "        else:\n",
    "            if (use_simple_loss):\n",
    "                loss = loss_Jdd_spread\n",
    "            else:\n",
    "                loss = loss_Jdd_spread + loss_Jdd_neg + loss_Tdd_vals0\n",
    "            \n",
    "        \n",
    "        return (loss, loss_Jdd, loss_Jdd_spread, loss_Jdd_neg, loss_J_iso,\n",
    "                loss_Tdd, loss_Tdd_vals0, loss_T_iso, \n",
    "                loss_dJdt, loss_Os_spread, loss_direc, loss_LinAl, \n",
    "                Jdd_mean, Jdd_spread)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Convenience functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convenience function for learning rate reduction: \n",
    "def reduce_learning_rate(new_lr, YourOptimizer):\n",
    "    for param_group in YourOptimizer.param_groups:\n",
    "        param_group['lr'] = new_lr\n",
    "\n",
    "        \n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "# On the fly visualization\n",
    "from IPython import display\n",
    "def generate_image(fig, history,best_epoch):\n",
    "    \"\"\"\n",
    "    Generates a plot during runtime \n",
    "    \"\"\"\n",
    "    \n",
    "    #fig = figure(figsize=(16,4))  \n",
    "    fig.clf()\n",
    "    \n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    ymin1=min(history[:,1])\n",
    "    ymin2=min(history[:,2])\n",
    "    ymax1=np.median(history[:,1])\n",
    "    ymax2=np.median(history[:,2])\n",
    "    ymin=min(ymin1,ymin2)\n",
    "    ymax=max(ymax1,ymax2)\n",
    "    dy=ymax-ymin\n",
    "    ymin=ymin-dy*0.05\n",
    "    ymax=ymax+dy*0.05\n",
    "    ax.set_ylim([0.0,1.05*max(history[:,1])])\n",
    "    ax.plot(history[:,0], history[:,1],'.-',label=r'train loss')\n",
    "    ax.plot(history[:,0], history[:,2],'--',label=r'validation loss')\n",
    "    ax.set_xlabel(r'epoch')\n",
    "    ax.set_ylabel(r'Loss')\n",
    "    \n",
    "    ax.scatter(history[best_epoch-1,0], history[best_epoch-1,2],c='r',s=40)\n",
    "    \n",
    "    ax.legend()\n",
    "    \n",
    "    ax2 = ax.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "    ax2.set_ylim([ymin,ymax])\n",
    "    ax2.plot(history[:,0], history[:,1],'.-',c='blue')\n",
    "    ax2.plot(history[:,0], history[:,2],'.-',c='red')\n",
    "\n",
    "    ax2.set_ylabel('Loss', color='blue')  # we already handled the x-label with ax1\n",
    "    ax2.tick_params(axis='y', labelcolor='blue')\n",
    "    ax2.scatter(history[best_epoch-1,0], history[best_epoch-1,2],c='k',s=40)\n",
    "    \n",
    "    \n",
    "    fig.suptitle('Epoch:: {}, best valLoss::{:.6f}'.format(epoch,history[:,2].min()), size=20)\n",
    "    \n",
    "    \n",
    "    \n",
    "    display.clear_output(wait=True)\n",
    "    display.display(gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convenience function to get test loss \n",
    "def eval_test_loss(tnet, batch_size, epoch, randomize=False, training=False):\n",
    "    tnet.eval() # Make evaluation mode, for correct batchnorm behaviour \n",
    "\n",
    "    if training:\n",
    "        dataset_val = AstroDataset(fitsfile=fitsdata_name,mode='train',randomize=randomize)\n",
    "    else:\n",
    "        dataset_val = AstroDataset(fitsfile=fitsdata_name,mode='validation',randomize=randomize)\n",
    "        \n",
    "    datagen_val = torch.utils.data.DataLoader(dataset_val,batch_size=batch_size,shuffle=False,drop_last=True,num_workers=num_workers)\n",
    "\n",
    "\n",
    "    loss_list = []\n",
    "    loss_list_Jd = []\n",
    "    loss_list_Td = []\n",
    "    for i, data in enumerate(datagen_val):\n",
    "        # get the inputs\n",
    "        inputs, Jlabels, Tlabels = data\n",
    "        inputs = inputs.to(device)\n",
    "        Jlabels = Jlabels.to(device)\n",
    "        Tlabels = Tlabels.to(device)\n",
    "\n",
    "        \n",
    "        optimizer.zero_grad() \n",
    "        inputs.requires_grad=True\n",
    "        Jlabels.requires_grad=True\n",
    "\n",
    "        \n",
    "        (loss, loss_Jd, loss_Jd_spread, loss_Jd_neg, loss_J_iso,\n",
    "         loss_Td, loss_Td_vals0, loss_T_iso, \n",
    "         loss_dJdt, loss_Os_spread, loss_direc, loss_LinAl, \n",
    "         Jdd_mean, Jdd_spread) = net(inputs,Jlabels,Tlabels,i,epoch)\n",
    "\n",
    "        loss_list.append(loss.item())\n",
    "        loss_list_Jd.append(loss_Jd.item())\n",
    "        loss_list_Td.append(loss_Td.item())\n",
    "\n",
    "    loss_list_np = np.array(loss_list)\n",
    "    loss_list_np[np.isnan(loss_list_np)]=1.0e6\n",
    "\n",
    "    loss_list_Jd_np = np.array(loss_list_Jd)\n",
    "    loss_list_Td_np = np.array(loss_list_Td)\n",
    "\n",
    "    return np.mean(loss_list_np), np.mean(loss_list_Jd_np), np.mean(loss_list_Td_np)\n",
    "\n",
    "\n",
    "\n",
    "# Convenience function to get predictions     \n",
    "def get_preds(tnet,batch_size,mode='all',randomize=False):\n",
    "    tnet.eval() # Make evaluation mode, for correct batchnorm behaviour \n",
    "\n",
    "    dataset_val = AstroDataset(fitsfile=fitspred_name,mode=mode,randomize=randomize)\n",
    "    datagen_val = torch.utils.data.DataLoader(dataset_val,batch_size=batch_size,shuffle=False)\n",
    "    \n",
    "    \n",
    "    preds = []\n",
    "    preds_spread = []\n",
    "    epoch = 0\n",
    "    for i, data in enumerate(datagen_val):\n",
    "        # get the inputs\n",
    "        inputs, Jlabels, Tlabels = data\n",
    "        inputs = inputs.to(device)\n",
    "        Jlabels=Jlabels.to(device)\n",
    "        Tlabels=Tlabels.to(device)\n",
    "\n",
    "        (loss, loss_Jd, loss_Jd_spread, loss_Jd_neg, loss_J_iso,\n",
    "         loss_Td, loss_Td_vals0, loss_T_iso, \n",
    "         loss_dJdt, loss_Os_spread, loss_direc, loss_LinAl, \n",
    "         Jdd_mean, Jdd_spread) = net(inputs,Jlabels,Tlabels,i,epoch)\n",
    "\n",
    "        preds += [Jdd_mean.data.cpu().numpy()]\n",
    "        preds_spread += [Jdd_spread.data.cpu().numpy()]\n",
    "\n",
    "    preds = np.concatenate(preds,axis=0)\n",
    "    preds_spread = np.concatenate(preds_spread,axis=0)\n",
    "\n",
    "    return preds, preds_spread\n",
    "\n",
    "\n",
    "\n",
    "# Convenience function to make and write out the predictions     \n",
    "def make_and_write_preds(tnet, batch_size):\n",
    "    import os\n",
    "\n",
    "    preds, preds_spread = get_preds(tnet,batch_size)\n",
    "\n",
    "    tab = fits.open(fitspred_name)\n",
    "\n",
    "    labels = tab[1].header\n",
    "    raw_data = tab[1].data\n",
    "        \n",
    "    labels_list = []\n",
    "    for i in range(0,len(labels)):\n",
    "        if 'TTYPE'+str(i) in labels:\n",
    "            labels_list.append(labels['TTYPE'+str(i)])\n",
    "\n",
    "        \n",
    "    table = Table(raw_data,names=labels_list)\n",
    "    n = len(table)\n",
    "        \n",
    "    copy_data = table.copy()\n",
    "\n",
    "\n",
    "    J1_preds=preds[:,0]*J_scale\n",
    "    J2_preds=preds[:,1]*J_scale\n",
    "    J3_preds=preds[:,2]*J_scale\n",
    "\n",
    "    J1_spread=preds_spread[:,0]*J_scale\n",
    "    J2_spread=preds_spread[:,1]*J_scale\n",
    "    J3_spread=preds_spread[:,2]*J_scale\n",
    "\n",
    "    colJ1_preds = Column(J1_preds, name = 'J1_pred')\n",
    "    colJ2_preds = Column(J2_preds, name = 'J2_pred')\n",
    "    colJ3_preds = Column(J3_preds, name = 'J3_pred')\n",
    "\n",
    "    colJ1_spread = Column(J1_spread, name = 'J1_spread')\n",
    "    colJ2_spread = Column(J2_spread, name = 'J2_spread')\n",
    "    colJ3_spread = Column(J3_spread, name = 'J3_spread')\n",
    "\n",
    "    copy_data.add_column(colJ1_preds)\n",
    "    copy_data.add_column(colJ2_preds)\n",
    "    copy_data.add_column(colJ3_preds)\n",
    "\n",
    "    copy_data.add_column(colJ1_spread)\n",
    "    copy_data.add_column(colJ2_spread)\n",
    "    copy_data.add_column(colJ3_spread)\n",
    "    \n",
    "    if os.path.exists(fitspred_name_out):\n",
    "        os.remove(fitspred_name_out)\n",
    "\n",
    "    fits.writeto(fitspred_name_out, np.array(copy_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some more convenience functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_network_GF_G_part(m):\n",
    "    for name, p in m.named_parameters():\n",
    "        if \"encoder_GF_G.ultra.encoder\" in name:\n",
    "            #print(\"freezing GF_G\", name)\n",
    "            p.requires_grad = False\n",
    "\n",
    "            \n",
    "def freeze_network_GF_P_part(m):\n",
    "    for name, p in m.named_parameters():\n",
    "        if \"encoder_GF_P.ultra.encoder\" in name:\n",
    "            #print(\"freezing GF_P\", name)\n",
    "            p.requires_grad = False\n",
    "\n",
    "            \n",
    "def freeze_network_A_part(m):\n",
    "    for name, p in m.named_parameters():\n",
    "        if \"encoder_A.ultra.encoder\" in name:\n",
    "            #print(\"freezing A\", name)\n",
    "            p.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HyperParameters of the run and network definitions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "#device = 'cpu'\n",
    "\n",
    "net = ActionFinder().to(device)\n",
    "net.to(dtype)\n",
    "\n",
    "\n",
    "if (Fit_Acceleration):\n",
    "    net.apply(freeze_network_GF_G_part)\n",
    "    net.apply(freeze_network_GF_P_part)\n",
    "\n",
    "    print(\"Fitting the acceleration field\")\n",
    "else:\n",
    "    net.apply(freeze_network_A_part)\n",
    "    if (use_point_transformation):\n",
    "        print(\"also fitting a canonical point-transformation\")\n",
    "    else:\n",
    "        print(\"NOT using a point-transformation\")\n",
    "        net.apply(freeze_network_GF_P_part)\n",
    "    \n",
    "\n",
    "dataset_train  = AstroDataset(fitsfile=fitsdata_name,mode='train')\n",
    "datagen_train = torch.utils.data.DataLoader(dataset_train,batch_size=batch_size,shuffle=True,drop_last=True,num_workers=num_workers)\n",
    "\n",
    "\n",
    "criterion = torch.nn.L1Loss()\n",
    "cossimilarity = torch.nn.CosineSimilarity(dim=2)\n",
    "\n",
    "\n",
    "wd=0.0\n",
    "\n",
    "# Define Optimizer \n",
    "lr = 3.e-3\n",
    "optimizer  = torch.optim.Adam(net.parameters(), lr=lr,weight_decay=wd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(count_parameters(net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#net.load_state_dict(torch.load(flname_save))# Load best model first if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os.path\n",
    "import timeit\n",
    "import datetime\n",
    "\n",
    "fig = figure(figsize=(8,6)) # Change this for larger image \n",
    "\n",
    "history = [] # monitoring\n",
    "val_loss_criterion = 1.0e30 # Checkpointing: Something very large so it will be picked up upon first epoch \n",
    "\n",
    "lr_new = lr/10.\n",
    "reduce_learning_rate(lr_new,optimizer)\n",
    "\n",
    "\n",
    "loss_Jd_best = 0.0\n",
    "loss_Td_best = 0.0\n",
    "\n",
    "time_init = datetime.datetime.now()\n",
    "start_time_init = timeit.default_timer()\n",
    "for epoch in range(1,Epochs+1):  # loop over the dataset multiple times\n",
    "    start_time = timeit.default_timer()\n",
    "             \n",
    "\n",
    "\n",
    "    net.train() # Make in train mode \n",
    "    train_loss = 0.0\n",
    "    train_loss_Jd = 0.0\n",
    "    train_loss_Jd_spread = 0.0\n",
    "    train_loss_Jd_neg = 0.0\n",
    "    train_loss_J_iso = 0.0\n",
    "    train_loss_Td = 0.0\n",
    "    train_loss_Td_vals0 = 0.0\n",
    "    train_loss_T_iso = 0.0\n",
    "    train_loss_dJdt = 0.0\n",
    "    train_loss_Os_spread = 0.0\n",
    "    train_loss_direc = 0.0\n",
    "    train_loss_LinAl = 0.0\n",
    "\n",
    "    loss_list_epoch = []\n",
    "    \n",
    "    datagen_train = torch.utils.data.DataLoader(dataset_train,batch_size=batch_size,shuffle=True,drop_last=True,num_workers=num_workers)\n",
    "\n",
    "    for i, data in enumerate(datagen_train):\n",
    "        # get the inputs\n",
    "        inputs, Jlabels, Tlabels = data\n",
    "        inputs = inputs.to(device) # pass to GPU if available \n",
    "        Jlabels=Jlabels.to(device)\n",
    "        Tlabels=Tlabels.to(device)\n",
    "\n",
    "        \n",
    "        optimizer.zero_grad() \n",
    "        inputs.requires_grad=True\n",
    "        Jlabels.requires_grad=True\n",
    "\n",
    "        \n",
    "        (loss, loss_Jd, loss_Jd_spread, loss_Jd_neg, loss_J_iso,\n",
    "         loss_Td, loss_Td_vals0, loss_T_iso, \n",
    "         loss_dJdt, loss_Os_spread, loss_direc, loss_LinAl, \n",
    "         Jdd_mean, Jdd_spreadmad) = net(inputs,Jlabels,Tlabels,i,epoch)\n",
    "\n",
    "\n",
    "        loss.backward() # Calculate gradients \n",
    "                \n",
    "        optimizer.step() # update weights \n",
    "                \n",
    "        # print statistics\n",
    "        train_loss += loss.item()\n",
    "        train_loss_Jd += loss_Jd.item()\n",
    "        train_loss_Jd_spread += loss_Jd_spread.item()\n",
    "        train_loss_Jd_neg += loss_Jd_neg.item()\n",
    "        train_loss_J_iso += loss_J_iso.item()\n",
    "        train_loss_Td += loss_Td.item()\n",
    "        train_loss_Td_vals0 += loss_Td_vals0.item()\n",
    "        train_loss_T_iso += loss_T_iso.item()\n",
    "        train_loss_dJdt += loss_dJdt.item()\n",
    "        train_loss_Os_spread += loss_Os_spread.item()\n",
    "        train_loss_direc += loss_direc.item()\n",
    "        train_loss_LinAl += loss_LinAl.item()\n",
    "\n",
    "        \n",
    "        loss_list_epoch.append(loss.item())\n",
    "\n",
    "                                \n",
    "    # Evaluate accuracy after each training epoch \n",
    "    net.eval() # make network in evaluation mode\n",
    "    if (use_validation_set):\n",
    "        acc, loss_Jd_eval, loss_Td_eval = eval_test_loss(net, batch_size=batch_size, epoch=epoch)\n",
    "    else:\n",
    "        acc=train_loss/len(datagen_train)\n",
    "        loss_Jd_eval=train_loss_Jd/len(datagen_train)\n",
    "        loss_Td_eval=train_loss_Td/len(datagen_train)\n",
    "    \n",
    "    # ========== Checkpointing ===========================\n",
    "    if acc < val_loss_criterion:\n",
    "        best_epoch = copy(epoch)\n",
    "        print(best_epoch, flname_save)\n",
    "        val_loss_criterion = acc        \n",
    "        torch.save(net.state_dict(), flname_save)\n",
    "        # see https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "        loss_Jd_best = np.copy(loss_Jd_eval)\n",
    "        loss_Td_best = np.copy(loss_Td_eval)\n",
    "        if (write_predictions_on_the_fly and epoch>100): # this might be expensive!\n",
    "            make_and_write_preds(net, batch_size=batch_size)        \n",
    "    # ======================================================\n",
    "        \n",
    "    history += [[epoch, train_loss/len(datagen_train),acc,loss_Jd_eval,loss_Td_eval]]\n",
    "    generate_image(fig,np.array(history),best_epoch)\n",
    "    print('Epoch::{} \\t best epoch::{} \\t train_loss::{:.6f} \\t val_loss::{:.6f}'.format(epoch, best_epoch, train_loss / len(datagen_train), acc))\n",
    "    print('train_loss_Jd        ::{:.6f} train_loss_Td        ::{:.6f}'.format(train_loss_Jd/len(datagen_train),train_loss_Td/len(datagen_train)))\n",
    "    print('train_loss_J_iso     ::{:.6f} train_loss_T_iso     ::{:.6f}'.format(train_loss_J_iso/len(datagen_train),train_loss_T_iso/len(datagen_train)))\n",
    "    print('train_loss_Jd_spread ::{:.6f} train_loss_Jd_neg    ::{:.6f}'.format(train_loss_Jd_spread/len(datagen_train),train_loss_Jd_neg/len(datagen_train)))\n",
    "    print('train_loss_Td_vals0  ::{:.6f}'.format(train_loss_Td_vals0/len(datagen_train)))\n",
    "    print('train_loss_dJdt      ::{:.6f} train_loss_direc     ::{:.6f}'.format(train_loss_dJdt/len(datagen_train),train_loss_direc/len(datagen_train)))\n",
    "    print('train_loss_Os_spread ::{:.6f} train_loss_LinAl     ::{:.6f}'.format(train_loss_Os_spread/len(datagen_train),train_loss_LinAl/len(datagen_train)))\n",
    "    print(' ')\n",
    "    print('loss_Jd_best         ::{:.6f} loss_Td_best         ::{:.6f}'.format(loss_Jd_best,loss_Td_best))\n",
    "\n",
    "    torch.set_printoptions(sci_mode=True,linewidth=None)\n",
    "    print('lr::{:.6f}'.format(optimizer.param_groups[0]['lr']))\n",
    "    torch.set_printoptions(sci_mode=None,linewidth=None)\n",
    "\n",
    "    \n",
    "    print()\n",
    "    time_now = timeit.default_timer()\n",
    "    elapsed_time = time_now - start_time\n",
    "    total_elapsed_time = time_now - start_time_init\n",
    "    print(\"elapsed time\",elapsed_time)\n",
    "    print(\"total   time\",total_elapsed_time/3600.0)\n",
    "    print()\n",
    "    \n",
    "    time_now = datetime.datetime.now()\n",
    "    time_next = time_now + datetime.timedelta(0,elapsed_time)\n",
    "\n",
    "    current_time = time_now.strftime(\"%H:%M:%S\")\n",
    "    next_time = time_next.strftime(\"%H:%M:%S\")\n",
    "\n",
    "    print(\"Starting time    :\", time_init)\n",
    "    print(\"Current  time    :\", current_time)\n",
    "    print(\"Expect   next at :\", next_time)\n",
    "\n",
    "\n",
    "print('Finished Training')\n",
    "history_pd = pd.DataFrame(history,columns=['Epoch','trainLoss','acc','loss_Jd_eval','loss_Td_eval'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions, concatenate predictions with original catalog, and write out to a fits file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (write_predictions_on_the_fly):\n",
    "    print(\"The predictions were already made during the running of the code.\")\n",
    "else:\n",
    "    net.load_state_dict(torch.load(flname_save))# Load best model first\n",
    "    make_and_write_preds(net, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store the history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Table.from_pandas(history_pd)\n",
    "t.write('history_DB_1024.fits', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
